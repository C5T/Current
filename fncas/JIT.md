## Native x64 opcodes-generating JIT

### Code

* Low level in `x64_native_jit/x64_native_jit.h` and `x64_native_jit/test.cc`.
* Mid level in `fncas/jit.h`.
* Some higher level in `fncas/optimize.h` and the very `test.cc` from this directory.

### Background

At some point, the "interpreted" gradient computation code became the bottleneck, and compiling the autogenerated source code, be it via `as`, or `nasm`, or `clang`, to `dlopen()` the generated function later became too slow.

I have done a bit of research around LLVM and simliar projects, quickly realized there is no simple and lightweight way to JIT-compile a mathematic expression into native machine opcodes, and hacked up one.

Generally, the generated assembly code is similar to what the already-existing assembly-generating logic does. All in all, it translates the AST tree of the mathematical expression of the function and its gradients directly into the machine represention of the set of instructions that performs those computations.

### Important details

While building the JIT component from scratch, I have made the following alterations to the already existing logic, and to what the "path of least resistance" may look like.

1. To avoid the problems with `@plt` calls, the signature of the very generated comptuting function simply takes an extra parameter: an array of pointers to external double-to-double functions. Calls to functions, both the standard `<cmath>` ones such as `log` or `exp` or `sin`, and FnCAS-provided `sqr` or `ramp`, are then performed as indirect calls by their indexes in this array of functions provided by the caller.

2. The original code that used to externally compile and `dlopen()` the computing function was providing the gradient-returning function with three arguments: a pointer to the the very variables vector, a pointer to the output gradient array, and a pointer to a "heap" of sufficient size, which this generated code is free to use while performing the computations. In this JIT-implementation, to reduce the number of parameters passed (and to reduce the number of different x64 opcodes to generate), the second and third parameters are combined: the output pointer is allocated to a sufficient size so that the first `dim` double values in it would be the value of the gradient, and immediately following these `dim` doubles is the memory block the computing code is free to use for its own purposes.

3. Some x64 instructions have different opcode lengths depending on how large the constant in their invocation is. To keep the code simple and eliminate any confusions, the input array of doubles, and output array of doubles, and the array of functions are shifted by an offset sufficient to make sure the user-level offset zero maps to the smallest opcode-level offset for of opcode of the maximum length.

### Calling conventions

Search for `opcodes::` in `x64_native_jit/test.cc` or in `fncas/jit.h` to quickly get to the very code generating pieces.

The conventions are:
* The function begins with `push rbx`, `mov rsi, rbx`, and and with `pop rbx`, `ret`.
* The pointer to the input array of doubles is passed in `rdi`.
* The pointer to the output (and intermediate) array of doubles is passed in `rsi` and stored in `rbx`.
* The pointer to the array of external mathematical functions is stored in `rdx`.
* When calling external functions, `rdi` and `rdx` are preserved on the stack.
* No need to preserve `rbx`, it is guaranteed to be unchanged (and the very generated code follows this convention).
* The only way to load immediate values used in the code generator is to load them directly into some output array element.
* The only double register used is `xmm0`,
* The register `xmm0` also contains the return value of the generated function.

More info: https://wiki.osdev.org/System_V_ABI

The arrays of input doubles and output doubles are shifted by 16 doubles (0x80 bytes) each. The array of user-provided functions is shifted by one function pointer (0x8 bytes).

There is a `FNCAS_DEBUG_NATIVE_JIT` symbol, which can be `#define`-d to make sure all the generated opcodes are dumped to stderr as preuso-code.

### Development notes

During developent, I have used the following or similar "canonical" C++ code (`f.cc`):

```
extern "C" double f(double const* x, double* y, double (*p[])(double x)) {
  static_cast<void>(y);
  return x[16+0] + x[16+1];
  // return p[1+0](x[16+0]) * x[16+1];
}
```

The following, or similar, "canonical" assembly code (`f.s`):

```
.text
.globl f

f:
  push   %rbx
  mov    %rsi, %rbx

  movsd  0x80(%rdi), %xmm0
  addsd  0x88(%rdi), %xmm0
  
  pop    %rbx
  retq
```

The following, or similar, example run code (`run.cc`):

```
#include <iostream>
#include <cmath>

extern "C" double f(double const *x, double *o, double (*f[])(double x));

int main() {
  double x[100];
  double o[100];
  double (*u[])(double x) = {exp, log};
  x[0] = 0.5;
  x[1] = 10;
  std::cout << f(x - 16, o - 16, u - 1) << std::endl;
  // May also dump the contents of `o[...]`. -- D.K.
}
```

And this `Makefile`:

```
.PHONY: all dump

all:
	g++ -c -O3 f.cc -o f.o
	#g++ -c f.s -o f.o
	g++ -c -O3 run.cc -o run.o
	g++ -O3 f.o run.o -o run && ./run
	rm run

dump: all
	objdump -S f.o
```

The default, `run` make target builds and runs this code.

The `make dump` command outputs the resulting assembly code.

If and when in doubt, I generated and tested the `f.o`code by compiling the `f.cc` source, then used `objdump -S` to understand it, and then comfirmed by understanding, first by compiling and running the same logic from the `f.s` code, and then, as necessary, by copy-pasting it into the `CopyPastedOpcodesCanBeExecuted` test in `x64_native_jit/test.cc`.
