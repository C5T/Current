TODO after adding real native Linux JIT:

* Memory optimizations:
  * Shrink the nodes container once the function is ready to be processed.
  * Do a dry run to `.reserve()` just the right number of bytes to `mmap()`, and then generate the ultimate code just there.
* Extra debug outputs.
  * Time it took to differentiate.
  * Size of the "binary" JIT code.
* Extra optimizations:
  * Actually operate on a `double*`, not a `vector<double>`, when the gradient is computed.
  * At least, do not add zeroes and multiply by ones in the gradient.
  * Collapse constants.
  * Remove multiplications by zero altogether.
* Look into how hard would it be to have this code run on a Mac as well.

"Good to have"-s and "didn't-have-time-to-add"-s, in no particular order.

* Hash of function. Use it in source / library file name. Don't regenerate / recompile same function.
* Variable policies during optimization (ex. this var should always be positive).
* Variable adjustment policies (ex. normalize this exp-normal simples to average of zero).
* Return `PointAndValue` from a compiled function-plus-gradient (now returns only the gradient)?
* Consider friendler syntax to a) not require explicit template type, and b) support lambdas.
